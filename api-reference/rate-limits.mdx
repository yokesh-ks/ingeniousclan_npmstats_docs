---
title: 'Rate Limits'
description: 'Understanding and managing API rate limits'
---

## Overview

To ensure fair usage and maintain service quality, NpmStats implements rate limiting on API requests. Limits vary based on your subscription tier and are applied on both a per-minute and per-day basis.

## Rate Limit Tiers

### Plan Limits

<CardGroup cols={2}>
  <Card title="Free Tier" icon="gauge-simple">
    - 1,000 requests per day
    - 60 requests per minute
    - Basic endpoints only
  </Card>
  <Card title="Pro Tier" icon="gauge-high">
    - 10,000 requests per day
    - 300 requests per minute
    - All endpoints
  </Card>
  <Card title="Enterprise Tier" icon="gauge-max">
    - Custom request limits
    - Priority processing
    - Dedicated support
  </Card>
  <Card title="Open Source" icon="code">
    - 2,000 requests per day
    - 120 requests per minute
    - Verified projects only
  </Card>
</CardGroup>

## Monitoring Usage

### Rate Limit Headers

All API responses include headers to help you track your rate limit usage:

```bash
X-RateLimit-Limit: 60           # Requests allowed per window
X-RateLimit-Remaining: 59       # Requests remaining in current window
X-RateLimit-Reset: 1573581600   # Timestamp when the limit resets
```

### Checking Current Usage

```typescript
// Get current rate limit status
const response = await fetch('https://api.npmstats.ingeniousclan.com/v1/rate-limit', {
  headers: {
    'Authorization': 'Bearer your-api-key'
  }
});

const limits = await response.json();
console.log(limits);
```

Example Response:
```json
{
  "rate": {
    "limit": 60,
    "remaining": 59,
    "reset": 1573581600
  },
  "daily": {
    "limit": 1000,
    "remaining": 999,
    "reset": 1573624800
  }
}
```

## Handling Rate Limits

### Rate Limit Errors

When you exceed the rate limit, you'll receive a `429 Too Many Requests` response:

```json
{
  "error": {
    "code": "rate_limit_exceeded",
    "message": "Rate limit exceeded. Please try again in 60 seconds.",
    "status": 429
  }
}
```

### Best Practices

<AccordionGroup>
  <Accordion title="Request Management">
    - Implement request queuing
    - Add retry logic with backoff
    - Cache responses when possible
  </Accordion>
  
  <Accordion title="Optimization">
    - Batch requests when possible
    - Use conditional requests
    - Implement response caching
  </Accordion>
  
  <Accordion title="Monitoring">
    - Track rate limit headers
    - Monitor usage patterns
    - Set up alerts
  </Accordion>
</AccordionGroup>

## Implementation Examples

### Retry Logic

```typescript
async function fetchWithRetry(url: string, maxRetries = 3) {
  for (let i = 0; i < maxRetries; i++) {
    try {
      const response = await fetch(url, {
        headers: {
          'Authorization': 'Bearer your-api-key'
        }
      });
      
      if (response.status === 429) {
        const resetTime = response.headers.get('X-RateLimit-Reset');
        const waitTime = Math.max(resetTime - Date.now(), 1000);
        await new Promise(resolve => setTimeout(resolve, waitTime));
        continue;
      }
      
      return response;
    } catch (error) {
      if (i === maxRetries - 1) throw error;
    }
  }
}
```

### Request Queuing

```typescript
class RequestQueue {
  private queue: Array<() => Promise<any>> = [];
  private processing = false;

  async add<T>(request: () => Promise<T>): Promise<T> {
    return new Promise((resolve, reject) => {
      this.queue.push(async () => {
        try {
          const result = await request();
          resolve(result);
        } catch (error) {
          reject(error);
        }
      });
      
      if (!this.processing) {
        this.process();
      }
    });
  }

  private async process() {
    this.processing = true;
    
    while (this.queue.length > 0) {
      const request = this.queue.shift();
      if (request) {
        await request();
        // Wait 1 second between requests
        await new Promise(resolve => setTimeout(resolve, 1000));
      }
    }
    
    this.processing = false;
  }
}
```

## Rate Limit Strategies

### Caching

<Steps>
  <Step title="Implement Cache">
    Set up response caching
  </Step>
  <Step title="Define TTL">
    Set appropriate cache durations
  </Step>
  <Step title="Use Conditionals">
    Implement If-Modified-Since headers
  </Step>
  <Step title="Invalidate">
    Clear cache when needed
  </Step>
</Steps>

### Request Optimization

<CardGroup cols={2}>
  <Card title="Batching" icon="layer-group">
    - Combine multiple requests
    - Use bulk endpoints
    - Reduce API calls
  </Card>
  <Card title="Caching" icon="database">
    - Local storage
    - Memory cache
    - CDN caching
  </Card>
</CardGroup>

## Enterprise Solutions

### Custom Limits

For enterprise customers, we offer:

1. Custom rate limits
2. Dedicated infrastructure
3. Priority processing
4. Custom caching solutions

### Enterprise Support

<CardGroup cols={2}>
  <Card title="Technical Support" icon="headset">
    - 24/7 assistance
    - Priority response
    - Custom solutions
  </Card>
  <Card title="Monitoring" icon="chart-line">
    - Usage analytics
    - Performance metrics
    - Custom reports
  </Card>
</CardGroup>

## Next Steps

<CardGroup cols={2}>
  <Card title="API Endpoints" icon="code" href="/api-reference/endpoints/package-stats">
    Explore available endpoints
  </Card>
  <Card title="Best Practices" icon="book" href="/guides/api-best-practices">
    Learn API best practices
  </Card>
</CardGroup> 